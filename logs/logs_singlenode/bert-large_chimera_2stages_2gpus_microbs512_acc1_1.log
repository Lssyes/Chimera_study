[Running nsys_wrap.sh rank: 1]
output----bert_prof/bert-large_chimera_2stages_2gpus_microbs512_acc1_node_rank1
Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.
当前时间: 2024-06-07 00:01:38
tcp://192.168.0.5:1234
1
local_rank 0, local_size 1, rank-1, world_size-2
[[stage_to_ranks]] -> {0: [0, 1], 1: [1, 0]}
[[[[[[[[[[999999]]]]]]]]]]
[[[[[[[[[[999999]]]]]]]]]]
[total_num_samples_per_step: 1024]]]]]]]]]]
[max_steps_per_epoch:        976]]]]]]]]]]
total_num_samples: 8192 num_epochs: 1
[[pytorch sync]] 0 epoch
[[Chimera start]] 0 epoch
[<pipeline.PipelineStage object at 0x7f02c5777fd0>, <pipeline.PipelineStage object at 0x7f02c4176550>]
[[pass of the dist.barrier()]]
[0;36;40m[[[_call_chimera_pipeline]]][0m
Traceback (most recent call last):
  File "./main_bert.py", line 501, in <module>
    main()
  File "./main_bert.py", line 111, in main
    train_one_epoch(epoch, 
  File "./main_bert.py", line 154, in train_one_epoch
    loss = stage.call_pipeline(train_iterator,
  File "/opt/conda/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/Chimera/pipeline.py", line 411, in call_pipeline
    _call_pipeline(**kwargs)
  File "/workspace/Chimera/pipeline.py", line 575, in _call_chimera_pipeline
    forward(index, up_down)
  File "/workspace/Chimera/pipeline.py", line 537, in forward
    call('call_forward', index, down_or_up,
  File "/workspace/Chimera/pipeline.py", line 534, in call
    getattr(self.pipe_stage[index], func_name)(*args)
  File "/workspace/Chimera/pipeline.py", line 257, in call_forward
    outputs = self.stage_module(**inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/workspace/Chimera/bert_model.py", line 69, in forward
    outputs = super().forward(
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 1022, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 612, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 497, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 427, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 325, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
RuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 39.42 GiB total capacity; 37.69 GiB already allocated; 223.06 MiB free; 37.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The target application returned non-zero exit code 1

The application terminated before the collection started. No report was generated.
Training script completed at Fri Jun  7 00:01:57 +03 2024
nsys script completed at Fri Jun  7 00:02:27 +03 2024
