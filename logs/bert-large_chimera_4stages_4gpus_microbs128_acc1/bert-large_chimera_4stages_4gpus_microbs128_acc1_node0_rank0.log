[Running nsys_wrap.sh rank: 0]
output----bert_prof/bert-large_chimera_4stages_4gpus_microbs128_acc1_node_rank0
python: /usr/bin/python3.8 /usr/lib/python3.8 /usr/lib/python3.9 /usr/lib/python2.7 /etc/python3.8 /usr/local/lib/python3.8 /opt/conda/bin/python3.8 /opt/conda/bin/python /opt/conda/bin/python3.8-config
/workspace/Chimera
Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.
当前时间: 2024-06-09 22:45:07
tcp://192.168.0.5:1234
0
local_rank 0, local_size 2, rank-0, world_size-4
[[stage_to_ranks]] -> {0: [0, 3], 1: [1, 2], 2: [2, 1], 3: [3, 0]}
[[[[[[[[[[9999]]]]]]]]]]
[[[[[[[[[[9999]]]]]]]]]]
[total_num_samples_per_step: 512]]]]]]]]]]
[max_steps_per_epoch:        19]]]]]]]]]]
total_num_samples: 4096 num_epochs: 1
============================
pipeline_method: chimera
num_epochs: 1
num_optimization_steps: 8
world_size: 4
num_replica: 2
num_pipeline: 2
num_micro_batches_per_step: 2
recompute: False
stage0: ranks [0, 3]
stage1: ranks [1, 2]
stage2: ranks [2, 1]
stage3: ranks [3, 0]
----------------------------
corpus_path: ./bert_data/wikipedia.segmented.nltk.txt
corpus_lines: 10000
vocab_path: ./bert_data/bert-large-uncased-vocab.txt
on_memory: False
do_lower_case: True
bert_config_path: ./configs/bert_config_bert-large-uncased.json
max_seq_length: 128
micro_batch_size: 128
num_optimization_steps: 8
num_epochs: None
gradient_accumulation_steps: 1
adam_learning_rate: 3e-05
adam_max_grad_norm: 1.0
beta1: 0.9
weight_decay: 0.01
warmup_proportion: 0.1
damping: 0.01
pipeline_method: chimera
chunks: 2
recompute: False
num_stages: 4
num_pipelines: 2
checkpoint_dir: None
save_checkpoint_steps: 200
seed: 1
p2p_backend: gloo
collective_backend: nccl
num_workers: 4
profile: False
observe_norm: False
log_interval: 1
config: None
wandb: False
============================
[[pytorch sync]] 0 epoch
[[Chimera start]] 0 epoch
[<pipeline.PipelineStage object at 0x7fd6c5d558e0>, <pipeline.PipelineStage object at 0x7fd6c5d55be0>]
shiiitttt epoch-0 step-0 num_steps_for_this_epoch-8 num_p2p_comm-16
/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:909: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
barrier1
barrier2
barrier3
[0;31;40m[epoch-0 step-0 start][0m
[[pass of the dist.barrier()]]
[0;36;40m[[[_call_chimera_pipeline]]][0m
[0;31;40m[g.device:] cuda:0[0m
[0;31;40m[g.device:] cuda:0[0m
epoch1 step1 loss = 11.203675270080566
[0;31;40m[epoch-0 step-0 start][0m
[[pass of the dist.barrier()]]
[0;36;40m[[[_call_chimera_pipeline]]][0m
Traceback (most recent call last):
  File "./main_bert.py", line 512, in <module>
    main()
  File "./main_bert.py", line 111, in main
    train_one_epoch(epoch, 
  File "./main_bert.py", line 165, in train_one_epoch
    loss = stage.call_pipeline(train_iterator,
  File "/opt/conda/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/Chimera/pipeline.py", line 411, in call_pipeline
    _call_pipeline(**kwargs)
  File "/workspace/Chimera/pipeline.py", line 577, in _call_chimera_pipeline
    backward(index, up_down)
  File "/workspace/Chimera/pipeline.py", line 541, in backward
    call('call_backward', index, down_or_up,
  File "/workspace/Chimera/pipeline.py", line 534, in call
    getattr(self.pipe_stage[index], func_name)(*args)
  File "/workspace/Chimera/pipeline.py", line 305, in call_backward
    torch.autograd.backward(out_tensors, grad_tensors=grad_tensors)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 1.86 GiB (GPU 0; 39.42 GiB total capacity; 35.23 GiB already allocated; 293.06 MiB free; 36.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The target application returned non-zero exit code 1

The application terminated before the collection started. No report was generated.
Training script completed at Sun Jun  9 22:45:27 +03 2024
scp: /workspace/Chimera/bert_prof/bert-large_chimera_4stages_4gpus_microbs32_acc1_rank3.qdrep: No such file or directory
scp: /workspace/Chimera/bert_prof/bert-large_chimera_4stages_4gpus_microbs32_acc1_rank3.sqlite: No such file or directory
nsys script completed at Sun Jun  9 22:47:17 +03 2024
