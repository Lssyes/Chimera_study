\n\n\n________________________________________________________________________________________
[Running nsys_wrap.sh rank: 1]
$NSYS_OUTPUT -> /workspace/Chimera/bert_prof/tmp/bert-large_chimera_4stages_4gpus_microbs100_acc1
Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.
当前时间: 2024-06-16 18:14:45
tcp://192.168.0.5:1234
1
local_rank 1, local_size 2, rank-1, world_size-4
[[stage_to_ranks]] -> {0: [0, 3], 1: [1, 2], 2: [2, 1], 3: [3, 0]}
[[[[[[[[[[9999]]]]]]]]]]
[[[[[[[[[[9999]]]]]]]]]]
[total_num_samples_per_step: 400]]]]]]]]]]
[max_steps_per_epoch:        24]]]]]]]]]]
total_num_samples: 3200 num_epochs: 1
[[pytorch sync]] 0 epoch
[[Chimera start]] 0 epoch
[<pipeline.PipelineStage object at 0x7fd3af243790>, <pipeline.PipelineStage object at 0x7fd3ac147fd0>]
shiiitttt epoch-0 step-0 num_steps_for_this_epoch-8 num_p2p_comm-16
barrier1
barrier2
barrier3
[0;31;40m[epoch-0 step-0 start][0m
[[pass of the dist.barrier()]]
[0;36;40m[[[_call_chimera_pipeline]]][0m
/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:909: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
Traceback (most recent call last):
  File "./main_bert.py", line 513, in <module>
    main()
  File "./main_bert.py", line 112, in main
    train_one_epoch(epoch, 
  File "./main_bert.py", line 166, in train_one_epoch
    loss = stage.call_pipeline(train_iterator,
  File "/opt/conda/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/workspace/Chimera/pipeline.py", line 450, in call_pipeline
    _call_pipeline(**kwargs)
  File "/workspace/Chimera/pipeline.py", line 614, in _call_chimera_pipeline
    forward(index, up_down)
  File "/workspace/Chimera/pipeline.py", line 576, in forward
    call('call_forward', index, down_or_up,
  File "/workspace/Chimera/pipeline.py", line 573, in call
    getattr(self.pipe_stage[index], func_name)(*args)
  File "/workspace/Chimera/pipeline.py", line 296, in call_forward
    outputs = self.stage_module(**inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/workspace/Chimera/bert_model.py", line 101, in forward
    outputs = self.encoder.forward(
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 612, in forward
    layer_outputs = layer_module(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 497, in forward
    self_attention_outputs = self.attention(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 427, in forward
    self_outputs = self.self(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 286, in forward
    mixed_query_layer = self.query(hidden_states)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1851, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:1! (when checking argument for argument mat2 in method wrapper_mm)

The target application returned non-zero exit code 1

The application terminated before the collection started. No report was generated.
Training script completed at Sun Jun 16 18:14:58 +03 2024, watching for nsys to complete {30s}...
nsys script completed at Sun Jun 16 18:15:28 +03 2024
